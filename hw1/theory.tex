\documentclass[a4paper]{article}

\usepackage[a4paper,top=1cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz-qtree}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{listings}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{graphicx}
\graphicspath{ {./} }

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\renewcommand{\thesubsubsection}{(\alph{subsubsection}).}

\title{Deep Learning Homework 1}

\author{Tao-Kang (Kevin) Chang}

\date{N14958031 / tc3149}

\begin{document}
\maketitle

\stepcounter{section}
\stepcounter{subsection}
\subsection{}
\subsubsection{} % -- 1.2.a --
1. Feed forward to get the logits. Feed the input $\bm{x}$ to the network, after first layer, we get $\bm{h} = f(\bm{W}^{(1)}\bm{x}+\bm{b}^{(1)})$; after second layer we get $\bm{\hat{y}} = g(\bm{W}^{(2)}\bm{h}+\bm{b}^{(2)})$. \\
2. Compute the loss. $\ell_{\text{MSE}}(\bm{\hat{y}}, \bm{y}) = \norm{\bm{\hat{y}}-\bm{y}}^2$. \\
3. Zero the gradients. Clear the gradient before running the backward pass because PyTorch accumulate gradients. \\
4. Back propagation. Compute the gradient of loss with respect to learnable parameters. \\
For $\bm{W}^{(2)}$ and $\bm{b}^{(2)}$, 
$$\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{W}^{(2)}}}=\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{W}^{(2)}}}$$
$$\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{b}^{(2)}}}=\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{b}^{(2)}}}$$
Similarly for $\bm{W}^{(1)}$ and $\bm{b}^{(1)}$,
$$\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{W}^{(1)}}}=\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{h}}}\cdot \frac{\partial{\bm{h}}}{\partial{\bm{W}^{(1)}}}$$
$$\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{b}^{(1)}}}=\frac{\partial{\ell_{\text{MSE}}}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{h}}}\cdot \frac{\partial{\bm{h}}}{\partial{\bm{b}^{(1)}}}$$
5. Stepping. According to the gradients computed in step 4 and the learning rate, update the parameters.

\subsubsection{} % -- 1.2.b --
From the problem description, we already know that $f(\cdot)=(\cdot)^+$, $g$ is identity function and $\ell_{\text{MSE}}(\bm{\hat{y}}, \bm{y}) = \norm{\bm{\hat{y}}-\bm{y}}^2$. Also, Linear$_{(i)}(\bm{x})=\bm{W}^{(i)}\bm{x}+\bm{b}^{(i)}$. For simplicity, I will use these symbols.
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Layer} & \textbf{Input} & \textbf{Output} \\
		\hline
		Linear$_1$ & $\bm{x}$ & $\bm{W}^{(1)}\bm{x}+\bm{b}^{(1)}$ \\
		\hline
		$f$ & Linear$_1$($\bm{x}$) & $f$(Linear$_1$($\bm{x}$)) \\
		\hline
		Linear$_2$ & $f$(Linear$_1$($\bm{x}$)) & $\bm{W}^{(2)}f(\text{Linear}_1(\bm{x}))+\bm{b}^{(2)}$ \\
		\hline
		$g$ & Linear$_2$($f$(Linear$_1$($\bm{x}$))) & $g$(Linear$_2$($f$(Linear$_1$($\bm{x}$)))) \\
		\hline
		Loss & $g$(Linear$_2$($f$(Linear$_1$($\bm{x}$)))), $\bm{y}$ & $\ell_{\text{MSE}}(g(\text{Linear}_2(f(\text{Linear}_1(\bm{x})))), \bm{y})$ \\
		\hline
	\end{tabular}
\end{center}

\subsubsection{} %-- 1.2.c --
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Parameter} & \textbf{Gradient} \\
		\hline
		$\bm{W}^{(1)}$ & $(\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}}\cdot \bm{W}^{(2)}\cdot \frac{\partial{\bm{z}_2}}{\partial{\bm{z}_1}})^T\cdot \bm{x}^T$ \\
		\hline
		$\bm{b}^{(1)}$ & $(\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}}\cdot \bm{W}^{(2)}\cdot \frac{\partial{\bm{z}_2}}{\partial{\bm{z}_1}})^T$ \\
		\hline
		$\bm{W}^{(2)}$ & $(\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}})^T\cdot {\bm{z}_2}^T$ \\
		\hline
		$\bm{b}^{(2)}$ & $(\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}\cdot \frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}})^T$ \\
		\hline
	\end{tabular}
\end{center}

\pagebreak
\subsubsection{} % -- 1.2.d --
Suppose $\bm{z}_2$ and $\bm{z}_1$ are size $m$ column vectors.
$$\frac{\partial{\bm{z}_2}}{\partial{\bm{z}_1}}=
\begin{bmatrix}
	\frac{\partial{z_{21}}}{\partial{z_{11}}} & \frac{\partial{z_{21}}}{\partial{z_{12}}} & \dots & \frac{\partial{z_{21}}}{\partial{z_{1m}}} \\\\
	\frac{\partial{z_{22}}}{\partial{z_{11}}} & \frac{\partial{z_{22}}}{\partial{z_{12}}} & \dots & \frac{\partial{z_{22}}}{\partial{z_{1m}}} \\\\
	\vdots & \vdots & \ddots & \vdots \\\\
	\frac{\partial{z_{2m}}}{\partial{z_{11}}} & \frac{\partial{z_{2m}}}{\partial{z_{12}}} & \dots & \frac{\partial{z_{2m}}}{\partial{z_{1m}}}
\end{bmatrix}$$
$$\bm{z_2}=\text{ReLU}(\bm{z_1})$$
$$z_{2i}=\begin{cases}
	z_{1i} \qquad & \text{if } z_{1i}>0 \\
	0 \qquad & \text{else}
\end{cases}$$
$$\frac{\partial{z_{2i}}}{\partial{z_{1j}}}=\begin{cases}
	0 \qquad & \text{if } i\neq j \\
	1 \qquad & \text{else if } z_{1i}>0 \\
	0 \qquad & \text{else}
\end{cases}$$
% ---------------
$\bm{\hat{y}}$ and $\bm{z}_3$ are size $k$ column vectors.
$$\frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}}=
\begin{bmatrix}
	\frac{\partial{\hat{y}_1}}{\partial{z_{31}}} & \frac{\partial{\hat{y}_1}}{\partial{z_{32}}} & \dots & \frac{\partial{\hat{y}_1}}{\partial{z_{3k}}} \\\\
	\frac{\partial{\hat{y}_2}}{\partial{z_{31}}} & \frac{\partial{\hat{y}_2}}{\partial{z_{32}}} & \dots & \frac{\partial{\hat{y}_2}}{\partial{z_{3k}}} \\\\
	\vdots & \vdots & \ddots & \vdots \\\\
	\frac{\partial{\hat{y}_k}}{\partial{z_{31}}} & \frac{\partial{\hat{y}_k}}{\partial{z_{32}}} & \dots & \frac{\partial{\hat{y}_k}}{\partial{z_{3k}}}
\end{bmatrix}$$
$$\bm{\hat{y}}=\bm{z_3}$$
$$\frac{\partial{\hat{y}_i}}{\partial{z_{3j}}}=\begin{cases}
	0 \qquad & \text{if }i\neq j \\
	1 \qquad & \text{else}
\end{cases}$$
% ---------------
$\ell$ is a scalar and $\bm{\hat{y}}$ is a size $k$ column vector.
$$\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}=
\begin{bmatrix}
	\frac{\partial{\ell}}{\partial{\hat{y}_1}} & \frac{\partial{\ell}}{\partial{\hat{y}_2}} & \dots & \frac{\partial{\ell}}{\partial{\hat{y}_k}}
\end{bmatrix}$$
$$\ell=\ell_{\text{MSE}}(\bm{\hat{y}}, \bm{y}) = \norm{\bm{\hat{y}}-\bm{y}}^2$$
$$\frac{\partial{\ell}}{\partial{\hat{y}_i}}=2(\hat{y}_i-y_i)$$

\subsection{}
\subsubsection{} % -- 1.3.a --
For 1.2.(b), we only need to change activation function $f$ from ReLU to Sigmoid, and change $g$ from Identity to Sigmoid. For example, the output of Loss would change from 
$$\ell_{\text{MSE}}(\text{Identity}(\text{Linear}_2(\text{ReLU}(\text{Linear}_1(\bm{x})))), \bm{y})$$
 to
$$\ell_{\text{MSE}}(\text{Sigmoid}(\text{Linear}_2(\text{Sigmoid}(\text{Linear}_1(\bm{x})))), \bm{y})$$
or if we consider $f$ and $g$ are variables, then nothing need to change,
$$\ell_{\text{MSE}}(g(\text{Linear}_2(f(\text{Linear}_1(\bm{x})))), \bm{y})$$
\\
For 1.2.(c), because Identity, ReLU and Sigmoid are all element-wise activation functions, nothing need to change. \\\\
For 1.2.(d), the dimensionality of $\frac{\partial{\bm{z}_2}}{\partial{\bm{z}_1}}$ and $\frac{\partial{\bm{\hat{y}}}}{\partial{\bm{z}_3}}$ wouldn't change, but the element $\frac{\partial{z_{2i}}}{\partial{z_{1j}}}$ and $\frac{\partial{\hat{y}_i}}{\partial{z_{3j}}}$ would change to 
$$\frac{\partial{z_{2i}}}{\partial{z_{1j}}}=\begin{cases}
	0 \qquad & \text{if } i\neq j \\
	\frac{exp(-z_{1i})}{(1+exp(-z_{1i}))^2} & \text{else}
\end{cases}$$
$$\frac{\partial{\hat{y}_i}}{\partial{z_{3j}}}=\begin{cases}
	0 \qquad & \text{if } i\neq j \\
	\frac{exp(-z_{3i})}{(1+exp(-z_{3i}))^2} & \text{else}
\end{cases}$$

\subsubsection{} % -- 1.3.b --
For 1.2.(b), we change $\ell_\text{MSE}$ to $\ell_\text{BCE}$. For example, the output of Loss would change from
$$\ell_{\text{MSE}}(g(\text{Linear}_2(f(\text{Linear}_1(\bm{x})))), \bm{y})$$
to
$$\ell_{\text{BCE}}(g(\text{Linear}_2(f(\text{Linear}_1(\bm{x})))), \bm{y})$$
or if we consider $\ell$ is variable of loss function, then nothing need to change,
$$\ell(g(\text{Linear}_2(f(\text{Linear}_1(\bm{x})))), \bm{y})$$
\\
For 1.2.(c), because both MSE and BCE take vector as input and output a scalar, nothing need to change. \\\\
For 1.2.(d), the dimensionality of $\frac{\partial{\ell}}{\partial{\bm{\hat{y}}}}$ wouldn't change, but the element $\frac{\partial{\ell}}{\partial{\hat{y}_i}}$ would change to
$$\frac{\partial{\ell}}{\partial{\hat{y}_i}}=-\frac{1}{K}(\frac{y_i}{\hat{y}_i}-\frac{1-y_i}{1-\hat{y}_i})$$

\subsubsection{} % -- 1.3.c --
Because Sigmoid($\cdot$) is between 0 and 1, the gradient gets smaller when network gets deeper, i.e. vanishing gradient becomes a problem. ReLU keeps a constant gradient when input $>0$, this could greatly mitigate the problem.

\end{document}